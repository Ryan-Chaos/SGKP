{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from dog import Dog\n",
    "from featurebooster import FeatureBooster\n",
    "\n",
    "orb_path = \"../extractors/orbslam2_features/lib\"\n",
    "sys.path.append(str(orb_path))\n",
    "from orbslam2_features import ORBextractor\n",
    "\n",
    "superpoint_path = \"../extractors/SuperPointPretrainedNetwork\"\n",
    "sys.path.append(str(superpoint_path))\n",
    "from demo_superpoint import SuperPointFrontend\n",
    "\n",
    "alike_path = \"../extractors/ALIKE\"\n",
    "sys.path.append(str(alike_path))\n",
    "import alike\n",
    "from alike import ALike\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "def normalize_keypoints(keypoints, image_shape):\n",
    "    \"\"\"\n",
    "    这个函数接受关键点(keypoints)和图像的尺寸(image_shape)作为输入，然后将关键点归一化\n",
    "    \"\"\"\n",
    "    x0 = image_shape[1] / 2\n",
    "    y0 = image_shape[0] / 2\n",
    "    scale = max(image_shape) * 0.7\n",
    "    kps = np.array(keypoints)\n",
    "    kps[:, 0] = (keypoints[:, 0] - x0) / scale\n",
    "    kps[:, 1] = (keypoints[:, 1] - y0) / scale\n",
    "    return kps \n",
    "\n",
    "def draw_matches(image1, match_kp1, image2, match_kp2):\n",
    "    \"\"\"\n",
    "    这个函数用于在一对图像上绘制匹配的关键点。它接受两个图像(image1 和 image2)和它们各自的匹配关键点(match_kp1 和 match_kp2)\n",
    "    \"\"\"\n",
    "    h1, w1 = image1.shape[:2]\n",
    "    h2, w2 = image2.shape[:2]\n",
    "    out_image = np.zeros((max(h1, h2), w1 + w2, 3), np.uint8) # 创建一个足够大的空白图像(out_image)来容纳两个图像并排显示\n",
    "    out_image[:h1, :w1, :] = image1\n",
    "    out_image[:h2, w1:w1+w2, :] = image2\n",
    "\n",
    "    green = (0, 255, 0)\n",
    "    blue = (0, 0, 255)\n",
    "    yellow = (0, 255, 255)\n",
    "    for (x1, y1), (x2, y2) in zip(match_kp1, match_kp2): # 使用循环遍历每对匹配的关键点，并在它们的位置上绘制黄色圆圈(yellow)和绿色连线(green)\n",
    "        cv2.circle(out_image, (int(x1), int(y1)), 6, yellow, -1)\n",
    "        cv2.circle(out_image, (int(x2) + w1, int(y2)), 6, yellow, -1)\n",
    "        cv2.line(out_image, (int(x1), int(y1)), (int(x2) + w1, int(y2)), green, 2)\n",
    "    return out_image"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "desc_type = \"superpoint\" # change the feature name here\n",
    "pair_idx = 3\n",
    "assert(pair_idx in [1, 2, 3])\n",
    "pair_path = os.path.join('img%d' % pair_idx)\n",
    "use_cuda = torch.cuda.is_available()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "img1 = cv2.imread(os.path.join(pair_path, '1.jfif'))\n",
    "img2 = cv2.imread(os.path.join(pair_path, '2.jfif'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# set feature extractor\n",
    "if desc_type.lower() in ['sift', 'rootsift', 'sosnet', 'hardnet']:\n",
    "        feature_extractor = Dog(descriptor=desc_type.lower())\n",
    "elif 'sift' in desc_type.lower():\n",
    "    # feature_extractor = Dog(descriptor='sift')\n",
    "    print(\"descriptor='sift'\")\n",
    "elif 'orb' in desc_type.lower():\n",
    "    # feature_extractor = ORBextractor(3000, 1.2, 8)\n",
    "    print(\"descriptor='orb'\")\n",
    "elif 'superpoint' in desc_type.lower():\n",
    "    sp_weights_path = \"../extractors/SuperPointPretrainedNetwork/superpoint_v1.pth\"\n",
    "    feature_extractor = SuperPointFrontend(weights_path=sp_weights_path, nms_dist=4, conf_thresh=0.015, nn_thresh=0.7, cuda=use_cuda)\n",
    "elif 'alike' in desc_type.lower():\n",
    "    # feature_extractor = ALike(**alike.configs['alike-l'], device='cuda' if use_cuda else 'cpu', top_k=-1, scores_th=0.2)\n",
    "    print(\"descriptor='alike'\")\n",
    "else:\n",
    "    raise Exception('Not supported descriptor: \"%s\".' % desc_type)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# set FeatureBooster\n",
    "if \"+Boost-\" in desc_type:\n",
    "    # load json config file 加载配置文件\n",
    "    config_file = \"../config.yaml\"\n",
    "    with open(str(config_file), 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(config[desc_type])\n",
    "\n",
    "    # Model 初始化模型\n",
    "    feature_booster = FeatureBooster(config[desc_type])\n",
    "    if use_cuda:\n",
    "        feature_booster.cuda()\n",
    "    feature_booster.eval()\n",
    "    # load the model 加载模型权重\n",
    "    model_path = str(\"../models/\" + desc_type + \".pth\")\n",
    "    print(model_path)\n",
    "    feature_booster.load_state_dict(torch.load(model_path))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# ---------------------看到这--------------------------extract得改\n",
    "def extract(image):\n",
    "    if 'alike' in desc_type.lower():\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        pred = feature_extractor(rgb, sub_pixel=True)\n",
    "        keypoints = pred['keypoints']\n",
    "        descriptors = pred['descriptors']\n",
    "        scores = pred['scores']\n",
    "        keypoints = np.hstack((keypoints, np.expand_dims(scores, 1)))\n",
    "    else:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        if 'superpoint' in desc_type.lower():\n",
    "            image = (image.astype('float32') / 255.)\n",
    "            keypoints, descriptors, _ = feature_extractor.run(image)\n",
    "            keypoints, descriptors = keypoints.T, descriptors.T\n",
    "        elif desc_type.lower() in ['sift', 'rootsift', 'sosnet', 'hardnet']:\n",
    "            image = (image.astype('float32') / 255.)\n",
    "            keypoints, scores, descriptors = feature_extractor.detectAndCompute(image)\n",
    "            keypoints = np.hstack((keypoints, np.expand_dims(scores, 1)))\n",
    "        elif 'sift' in desc_type.lower():\n",
    "            image = (image.astype('float32') / 255.)\n",
    "            keypoints, scores, descriptors = feature_extractor.detectAndCompute(image)\n",
    "        elif 'orb' in desc_type.lower():\n",
    "            kps_tuples, descriptors = feature_extractor.detectAndCompute(image)\n",
    "            # convert keypoints \n",
    "            keypoints = [cv2.KeyPoint(*kp) for kp in kps_tuples]\n",
    "            keypoints = np.array(\n",
    "                [[kp.pt[0], kp.pt[1], kp.size / 31, np.deg2rad(kp.angle)] for kp in keypoints], \n",
    "                dtype=np.float32\n",
    "            )\n",
    "\n",
    "    if \"+Boost-\" in desc_type:\n",
    "        # boosted the descriptor using trained model\n",
    "        kps = normalize_keypoints(keypoints, image.shape)\n",
    "        kps = torch.from_numpy(kps.astype(np.float32))\n",
    "        if 'orb' in desc_type.lower():\n",
    "            descriptors = np.unpackbits(descriptors, axis=1, bitorder='little')\n",
    "            descriptors = descriptors * 2.0 - 1.0\n",
    "        descriptors = torch.from_numpy(descriptors.astype(np.float32))\n",
    "        if use_cuda:\n",
    "            kps = kps.cuda()\n",
    "            descriptors = descriptors.cuda()\n",
    "        out = feature_booster(descriptors, kps)\n",
    "        if 'boost-b' in desc_type.lower():\n",
    "            out = (out >= 0).cpu().detach().numpy()\n",
    "            descriptors = np.packbits(out, axis=1, bitorder='little')\n",
    "        else:\n",
    "            descriptors = out.cpu().detach().numpy()\n",
    "\n",
    "    # convert the keypoint to the form of cv2.KeyPoint\n",
    "    keypoints = [cv2.KeyPoint(keypoints[i][0], keypoints[i][1], 2) for i in range(keypoints.shape[0])]\n",
    "\n",
    "    return keypoints, descriptors"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "kps1, descs1 = extract(img1)\n",
    "kps2, descs2 = extract(img2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "vis_img1 = cv2.drawKeypoints(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB), kps1, None, \n",
    "                            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "vis_img2 = cv2.drawKeypoints(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB), kps2, None, \n",
    "                            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(1,2,1), plt.imshow(vis_img1), plt.axis('off')\n",
    "plt.subplot(1,2,2), plt.imshow(vis_img2), plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# create BFMatcher object\n",
    "bf_float = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "bf_binary = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "# Match descriptors.\n",
    "if 'orb' in desc_type.lower() or '+Boost-B' in desc_type:\n",
    "    matches = bf_binary.match(descs1, descs2)\n",
    "else:\n",
    "    matches = bf_float.match(descs1, descs2)\n",
    "\n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "print('raw matches: ', len(matches))\n",
    "src_pts = np.float32([ kps1[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "dst_pts = np.float32([ kps2[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "\n",
    "# RANSAC using Homography\n",
    "M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 3.0)\n",
    "matchesMask = mask.ravel().tolist()\n",
    "print('inliers: ', mask.sum())\n",
    "\n",
    "# show the matches\n",
    "matches = [ matches[i] for i in range(len(matches)) if matchesMask[i]]\n",
    "src_pts = [ kps1[m.queryIdx].pt for m in matches ]\n",
    "dst_pts = [ kps2[m.trainIdx].pt for m in matches ]\n",
    "vis_img = draw_matches(img1, src_pts, img2, dst_pts)\n",
    "vis_img = cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(vis_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24293eb685027aff9d7e9b62d4252b11fbd82aacc5f7e332be036278e88fb5d8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('devFeatureCUDA11': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
